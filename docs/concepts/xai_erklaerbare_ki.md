---
layout: default
title: Methoden & Frameworks
parent: XAI
grand_parent: Konzepte
nav_order: 1
description: "Einf√ºhrung in Explainable AI (XAI): Grundkonzepte (Black-Box, Perturbation, Surrogate-Modelle) und Methoden (LIME, SHAP, ELI5, Counterfactuals, Anchors, InterpretML)"
has_toc: true
---

# Methoden & Frameworks
{: .no_toc }

> **Explainable AI (XAI) umfasst Methoden und Techniken, die ML-Modelle f√ºr Menschen verst√§ndlich und nachvollziehbar machen.** 

---

## Inhaltsverzeichnis
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Einf√ºhrung in XAI

### Was ist Explainable AI?

Explainable AI (XAI) ist ein Ansatz der k√ºnstlichen Intelligenz, der darauf abzielt, dass die Funktionsweise und Entscheidungen von ML-Modellen f√ºr Menschen verst√§ndlich und nachvollziehbar sind.

```mermaid
flowchart TD
    subgraph Problem["üî≤ Das Black-Box-Problem"]
        INPUT[/"Eingabedaten"/]
        MODEL[("ML-Modell<br/>(Black Box)")]
        OUTPUT[/"Vorhersage"/]
        INPUT --> MODEL --> OUTPUT
    end
    
    subgraph L√∂sung["‚úÖ XAI-L√∂sung"]
        INPUT2[/"Eingabedaten"/]
        MODEL2[("ML-Modell")]
        OUTPUT2[/"Vorhersage"/]
        EXPLAIN["üìä Erkl√§rung:<br/>Welche Features waren wichtig?<br/>Warum diese Entscheidung?"]
        INPUT2 --> MODEL2 --> OUTPUT2
        MODEL2 -.-> EXPLAIN
    end
    
    Problem --> L√∂sung
    
    style Problem fill:#ffcccc,stroke:#cc0000
    style L√∂sung fill:#ccffcc,stroke:#00cc00
    style EXPLAIN fill:#ffffcc,stroke:#cccc00
```

### Warum ist XAI wichtig?

Die Umsetzung von XAI-Methoden tr√§gt dazu bei, das Vertrauen in KI-Systeme zu erh√∂hen, indem sie Transparenz und Nachvollziehbarkeit in den Entscheidungsprozess bringen.

| Bereich | Bedeutung von XAI |
|---------|-------------------|
| **Medizin** | √Ñrzte m√ºssen verstehen, warum ein Modell eine Diagnose vorschl√§gt |
| **Finanzwesen** | Kreditentscheidungen m√ºssen gegen√ºber Kunden begr√ºndbar sein |
| **Rechtswesen** | Algorithmen m√ºssen den Anforderungen an Fairness und Nachvollziehbarkeit gen√ºgen |
| **Compliance** | DSGVO und andere Regularien fordern Erkl√§rbarkeit automatisierter Entscheidungen |

---

## Grundlegende Konzepte

Bevor wir die einzelnen XAI-Methoden betrachten, sollten einige zentrale Begriffe verstanden werden.

**Wichtige Fachbegriffe f√ºr dieses Kapitel:**

| **Begriff**           | **Bedeutung**                                                                                                               |
| --------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| **Approximation**     | Ann√§herung ‚Äì ein vereinfachtes Modell, das das Verhalten eines komplexen Modells _ungef√§hr_ nachbildet                      |
| **Modell-agnostisch** | Unabh√§ngig vom Modelltyp ‚Äì die Methode funktioniert bei jedem ML-Modell, egal ob neuronales Netz, Random Forest oder andere |
| **Feature**           | Ein Eingabemerkmal des Modells (z.B. Alter, Einkommen, Geschlecht)                                                          |
| **Scope**             | Geltungsbereich ‚Äì ob eine Erkl√§rung f√ºr eine einzelne Vorhersage (lokal) oder das gesamte Modell (global) gilt              |
### Black-Box-Modelle

Ein **Black-Box-Modell** ist ein ML-Modell, dessen interne Entscheidungslogik nicht direkt einsehbar oder interpretierbar ist. Man sieht nur Input und Output, aber nicht *wie* die Entscheidung zustande kommt.

| Modelltyp | Transparenz | Beispiele |
|-----------|-------------|-----------|
| **White-Box** | Vollst√§ndig interpretierbar | Lineare Regression, Decision Trees, Regelbasierte Systeme |
| **Grey-Box** | Teilweise interpretierbar | Ensemble-Methoden mit Feature Importance |
| **Black-Box** | Nicht direkt interpretierbar | Tiefe neuronale Netze, komplexe Ensemble-Modelle |

XAI-Methoden machen Black-Box-Modelle nachvollziehbar, ohne deren Architektur zu ver√§ndern.

### Perturbierte Samples

**Perturbierte Samples** sind Datenpunkte, die absichtlich leicht ver√§ndert (gest√∂rt) wurden. Der Begriff kommt vom lateinischen *perturbare* (durcheinanderbringen, st√∂ren).

```mermaid
flowchart LR
    subgraph Original["üìä Original-Datenpunkt"]
        O["Alter: 25<br/>Klasse: 1<br/>Geschlecht: m"]
    end
    
    subgraph Perturbiert["üîÄ Perturbierte Samples"]
        P1["Alter: 30<br/>Klasse: 1<br/>Geschlecht: m"]
        P2["Alter: 25<br/>Klasse: 2<br/>Geschlecht: m"]
        P3["Alter: 25<br/>Klasse: 1<br/>Geschlecht: w"]
        P4["Alter: 22<br/>Klasse: 3<br/>Geschlecht: m"]
    end
    
    Original -->|"Systematische<br/>Variation"| Perturbiert
    
    style Original fill:#e3f2fd,stroke:#1565c0
    style Perturbiert fill:#fff3e0,stroke:#e65100
```

**Grundprinzip in XAI:** Man ver√§ndert systematisch einzelne Features eines Inputs und beobachtet, wie sich die Modellvorhersage √§ndert. Gro√üe √Ñnderungen im Output deuten auf wichtige Features hin.

**XAI-Methoden, die Perturbation nutzen:**

| Methode | Art der Perturbation | Zweck |
|---------|---------------------|-------|
| **LIME** | Zuf√§llige Variation um einen Datenpunkt | Lokales Surrogate-Modell trainieren |
| **KernelSHAP** | Systematisches Maskieren von Feature-Kombinationen | Shapley-Werte approximieren |
| **Permutation Importance** | Zuf√§lliges Durchmischen einzelner Features | Globale Feature-Wichtigkeit messen |
| **Occlusion Sensitivity** | Verdecken von Bildbereichen | Wichtige Regionen in Bildern identifizieren |

**Vorteil der Perturbation:** Modell-Agnostik ‚Äì man braucht keinen Zugriff auf interne Gewichte, nur auf die Input-Output-Beziehung.

### Surrogate-Modelle

Ein **Surrogate-Modell** (auch Ersatzmodell) ist ein einfaches, interpretierbares Modell, das trainiert wird, um die Vorhersagen eines komplexen Black-Box-Modells nachzuahmen.

**Alltagsanalogie:** Stellen Sie sich einen erfahrenen Arzt vor, der Diagnosen stellt, aber nicht erkl√§ren kann, *warum* er zu diesem Schluss kommt ‚Äì er "sp√ºrt" es einfach nach 30 Jahren Erfahrung. Ein Surrogate-Modell w√§re wie ein Praktikant, der den Arzt bei vielen Diagnosen beobachtet und dann einfache Regeln ableitet: "Wenn Symptom A und B vorliegen, diagnostiziert der Arzt meist Krankheit X." Die Regeln des Praktikanten sind nicht perfekt, aber sie machen das Verhalten des Arztes nachvollziehbar.

```mermaid
flowchart TD
    subgraph BlackBox["üî≤ Black-Box-Modell"]
        BB["Neuronales Netz<br/>XGBoost<br/>Random Forest"]
    end
    
    subgraph Surrogate["üìê Surrogate-Modell"]
        SU["Lineare Regression<br/>Decision Tree<br/>Regelbasiertes System"]
    end
    
    DATA["Eingabedaten"] --> BlackBox
    BlackBox -->|"Vorhersagen als<br/>Trainingsdaten"| Surrogate
    Surrogate -->|"Interpretation der<br/>Koeffizienten/Regeln"| EXPLAIN["üìä Erkl√§rung"]
    
    style BlackBox fill:#ffcccc,stroke:#cc0000
    style Surrogate fill:#ccffcc,stroke:#00cc00
    style EXPLAIN fill:#ffffcc,stroke:#cccc00
```

| Surrogate-Typ | Scope | Methode |
|---------------|-------|---------|
| **Global** | Gesamtes Modell | Ein Surrogate erkl√§rt alle Vorhersagen |
| **Lokal** | Einzelne Vorhersage | LIME trainiert ein Surrogate nur f√ºr einen Datenpunkt |

**Wichtig:** Das Surrogate-Modell erkl√§rt nicht das Original-Modell selbst, sondern dessen *Verhalten* ‚Äì die Erkl√§rung ist eine Approximation.

---

## XAI-Ans√§tze im √úberblick

```mermaid
flowchart TD
    XAI["üîç XAI-Ans√§tze"]
    
    XAI --> IM["üìê Interpretable Models"]
    XAI --> LE["üéØ Local Explanation"]
    XAI --> GE["üåç Global Explanation"]
    
    IM --> IM1["Decision Trees"]
    IM --> IM2["Lineare Regression"]
    IM --> IM3["Regelbasierte Systeme"]
    
    LE --> LE1["LIME"]
    LE --> LE2["SHAP (lokal)"]
    LE --> LE3["Break-Down Analyse"]
    
    GE --> GE1["Feature Importance"]
    GE --> GE2["SHAP Summary"]
    GE --> GE3["Partial Dependence"]
    
    style XAI fill:#e1f5fe,stroke:#01579b
    style IM fill:#fff3e0,stroke:#e65100
    style LE fill:#e8f5e9,stroke:#2e7d32
    style GE fill:#fce4ec,stroke:#c2185b
```

### Interpretable Models

Verwendung von ML-Modellen, die von Grund auf so konzipiert sind, dass sie erkl√§rbar sind:

- **Decision Trees**: Klare Entscheidungsregeln, visuell darstellbar
- **Lineare Regression**: Koeffizienten zeigen direkt den Einfluss jedes Features
- **Regelbasierte Systeme**: Explizite IF-THEN-Regeln

### Local Explanation

Erkl√§rung individueller Vorhersagen durch Analyse der wichtigsten Features und ihrer Auspr√§gungen:

> **Beispiel**: Warum wurde f√ºr Passagier X vorhergesagt, dass er √ºberlebt?
> - Geschlecht: weiblich ‚Üí +45% √úberlebenschance
> - Klasse: 1. Klasse ‚Üí +20% √úberlebenschance
> - Alter: 22 Jahre ‚Üí +5% √úberlebenschance

### Global Explanation

Ganzheitliche Erkl√§rung der Prognosef√§higkeit eines ML-Modells:

- **Feature Importance**: Welche Merkmale sind insgesamt am wichtigsten?
- **Partial Dependence**: Wie beeinflusst ein Feature die Vorhersage √ºber alle Datenpunkte?
- **Accumulated Local Dependence**: Robustere Alternative zu Partial Dependence

---

## SHAP ‚Äì SHapley Additive exPlanations

### Konzept

SHAP basiert auf der Spieltheorie und dem Shapley-Wert, der urspr√ºnglich zur fairen Verteilung von Gewinnen in Koalitionen entwickelt wurde.

```mermaid
flowchart LR
    subgraph Spieltheorie["üé≤ Spieltheorie-Analogie"]
        P1["Spieler A"]
        P2["Spieler B"]
        P3["Spieler C"]
        GEWINN["üí∞ Gewinn"]
        P1 & P2 & P3 --> GEWINN
    end
    
    subgraph ML["ü§ñ ML-Kontext"]
        F1["Feature 1<br/>(Alter)"]
        F2["Feature 2<br/>(Geschlecht)"]
        F3["Feature 3<br/>(Klasse)"]
        PRED["üìä Vorhersage"]
        F1 & F2 & F3 --> PRED
    end
    
    Spieltheorie -.->|"√úbertragung"| ML
    
    style Spieltheorie fill:#e3f2fd,stroke:#1565c0
    style ML fill:#f3e5f5,stroke:#7b1fa2
```

### Berechnung des Shapley-Werts

Der Shapley-Wert ber√ºcksichtigt alle m√∂glichen Kombinationen von Features und berechnet den durchschnittlichen Beitrag jedes Features:

1. Betrachte alle m√∂glichen Teilmengen von Features
2. Berechne f√ºr jede Teilmenge die Vorhersage mit und ohne das Feature
3. Bilde den gewichteten Durchschnitt √ºber alle Kombinationen

### SHAP-Visualisierungen

| Visualisierung      | Beschreibung                                  | Scope  |
| ------------------- | --------------------------------------------- | ------ |
| **Waterfall Plot**  | Zeigt schrittweise den Beitrag jedes Features | Lokal  |
| **Force Plot**      | Kompakte Darstellung der Feature-Beitr√§ge     | Lokal  |
| **Summary Plot**    | √úbersicht √ºber alle Features und Datenpunkte  | Global |
| **Dependence Plot** | Einfluss eines Features auf die Vorhersage    | Global |

### Code-Beispiel

```python
import shap

# SHAP Explainer erstellen
explainer = shap.TreeExplainer(model)

# SHAP-Werte berechnen
shap_values = explainer(data_test)

# Waterfall Plot f√ºr einzelne Vorhersage
shap.plots.waterfall(shap_values[0])

# Summary Plot f√ºr globale √úbersicht
shap.plots.summary(shap_values)
```

---

## LIME ‚Äì Local Interpretable Model-agnostic Explanations

### Konzept

LIME erkl√§rt einzelne Vorhersagen, indem es ein einfaches, interpretierbares Modell lokal um die zu erkl√§rende Instanz herum trainiert.

```mermaid
flowchart TD
    subgraph LIME["LIME-Prozess"]
        INST["üéØ Zu erkl√§rende Instanz"]
        PERT["üîÑ Perturbierte Samples<br/>generieren"]
        WEIGHT["‚öñÔ∏è Gewichtung nach<br/>√Ñhnlichkeit"]
        LOCAL["üìê Lokales lineares<br/>Modell trainieren"]
        EXPLAIN["üìä Erkl√§rung<br/>extrahieren"]
        
        INST --> PERT --> WEIGHT --> LOCAL --> EXPLAIN
    end
    
    style INST fill:#ffeb3b,stroke:#f57f17
    style EXPLAIN fill:#4caf50,stroke:#2e7d32
```

### Funktionsweise

1. **Sample-Generierung**: Erzeuge √§hnliche Datenpunkte durch Perturbation
2. **Gewichtung**: Gewichte Samples nach N√§he zur Original-Instanz
3. **Lokales Modell**: Trainiere ein interpretierbares Modell (z.B. lineare Regression)
4. **Interpretation**: Die Koeffizienten des lokalen Modells erkl√§ren die Vorhersage

### Code-Beispiel

```python
from lime.lime_tabular import LimeTabularExplainer

# LIME Explainer erstellen
explainer = LimeTabularExplainer(
    training_data=data_train.values,
    feature_names=data_train.columns.tolist(),
    class_names=['Nicht √ºberlebt', '√úberlebt'],
    mode='classification'
)

# Erkl√§rung f√ºr einzelne Instanz
explanation = explainer.explain_instance(
    data_row=rose.values[0],
    predict_fn=model.predict_proba,
    num_features=5
)

# Visualisierung
explanation.show_in_notebook()
```

---

## ELI5 ‚Äì Explain Like I'm 5

### Konzept

ELI5 ist ein Framework, das Erkl√§rungen so einfach wie m√∂glich darstellt ‚Äì wie f√ºr ein 5-j√§hriges Kind. Es fokussiert auf Permutation Importance.

### Permutation Importance

Die Methode misst die Wichtigkeit eines Features, indem sie dessen Werte zuf√§llig permutiert und den Einfluss auf die Modellleistung beobachtet:

```mermaid
flowchart LR
    subgraph Original["üìä Original"]
        DATA1["Daten"]
        SCORE1["Score: 0.85"]
        DATA1 --> SCORE1
    end
    
    subgraph Permutiert["üîÄ Feature X permutiert"]
        DATA2["Daten<br/>(X gemischt)"]
        SCORE2["Score: 0.65"]
        DATA2 --> SCORE2
    end
    
    Original --> Permutiert
    Permutiert --> IMP["üìà Importance(X) = 0.85 - 0.65 = 0.20"]
    
    style IMP fill:#c8e6c9,stroke:#388e3c
```

### Code-Beispiel

```python
import eli5
from eli5.sklearn import PermutationImportance

# Permutation Importance berechnen
perm = PermutationImportance(model, random_state=42)
perm.fit(data_test, target_test)

# HTML-Darstellung
eli5.show_weights(perm, feature_names=data_test.columns.tolist())

# Feature-Gewichte des Modells
eli5.show_weights(model, feature_names=data_train.columns.tolist())
```

---

## InterpretML ‚Äì Microsoft Framework

### Konzept

InterpretML ist Microsofts umfassendes Open-Source-Framework f√ºr Explainable AI, das sowohl interpretierbare Modelle als auch Black-Box-Erkl√§rungen unterst√ºtzt.

### Kernfunktionen

| Funktion | Beschreibung |
|----------|--------------|
| **Explainable Boosting Machine (EBM)** | Interpretierbares Modell mit Boosting-Performance |
| **SHAP Kernel** | Black-Box-Erkl√§rungen f√ºr beliebige Modelle |
| **Interaktive Dashboards** | Web-basierte Visualisierungen |
| **Unified API** | Einheitliche Schnittstelle f√ºr verschiedene Erkl√§rungsmethoden |

### Code-Beispiel

```python
from interpret import show
from interpret.blackbox import ShapKernel

# SHAP Kernel Explainer
shap_explainer = ShapKernel(
    predict_fn=model.predict_proba,
    data=data_train,
    feature_names=data_train.columns.tolist()
)

# Lokale Erkl√§rung
local_explanation = shap_explainer.explain_local(
    X=rose,
    y=None,
    name="Rose"
)

# Interaktives Dashboard
show(local_explanation)
```

---

## Feature Importance (Random Forest)

### Konzept

Random Forest berechnet die Feature Importance basierend darauf, wie stark jedes Feature zur Reduktion der Unreinheit (Impurity) in den Entscheidungsb√§umen beitr√§gt.

### Vorteile

- **Schnell**: Direkt im Training berechnet, kein zus√§tzlicher Aufwand
- **Integriert**: In scikit-learn bereits enthalten
- **Einfach interpretierbar**: Direkte Rangfolge der Features

### Einschr√§nkungen

- Zeigt keine **Richtung** des Einflusses (positiv/negativ)
- Kann bei korrelierten Features irref√ºhrend sein
- Nur f√ºr Tree-basierte Modelle verf√ºgbar

### Code-Beispiel

```python
import pandas as pd
import plotly.express as px

# Feature Importance extrahieren
feature_importance = pd.DataFrame({
    'Feature': data_train.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

# Visualisierung
fig = px.bar(
    feature_importance,
    x='Importance',
    y='Feature',
    orientation='h',
    title='Feature Importance',
    color='Importance',
    color_continuous_scale='viridis'
)
fig.show()
```

---

## Framework-Vergleich

### √úbersichtstabelle

| Framework | St√§rken | Schw√§chen | Einsteigerfreundlichkeit |
|-----------|---------|-----------|--------------------------|
| **LIME** | Sehr intuitiv, gute Visualisierung, schnell lokal | Nur lokale Erkl√§rungen, kann instabil sein | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **SHAP** | Theoretisch fundiert, beste Visualisierungen, lokal & global | Kann langsam sein, komplexeres Konzept | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **ELI5** | Extrem einfach, minimaler Code, schnell | Weniger Visualisierungen, weniger Features | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **InterpretML** | Interaktive Dashboards, umfassend, professionell | Komplexer Setup, Overhead f√ºr einfache Aufgaben | ‚≠ê‚≠ê‚≠ê |
| **RF Importance** | Extrem schnell, in sklearn integriert | Nur Feature Importance, keine Richtung | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### Entscheidungshilfe

```mermaid
flowchart TD
    START["‚ùì Welches XAI-Framework?"]
    
    START --> Q1{"Einzelne Vorhersage<br/>oder gesamtes Modell?"}
    
    Q1 -->|"Einzelne<br/>Vorhersage"| Q2{"Schnelligkeit<br/>wichtig?"}
    Q1 -->|"Gesamtes<br/>Modell"| Q3{"Tree-basiertes<br/>Modell?"}
    
    Q2 -->|"Ja"| LIME["üîç LIME"]
    Q2 -->|"Nein"| SHAP_L["üéØ SHAP"]
    
    Q3 -->|"Ja"| RF["üå≤ RF Importance<br/>+ SHAP"]
    Q3 -->|"Nein"| Q4{"Interaktives<br/>Dashboard n√∂tig?"}
    
    Q4 -->|"Ja"| IML["üè¢ InterpretML"]
    Q4 -->|"Nein"| SHAP_G["üéØ SHAP Summary"]
    
    style START fill:#e3f2fd,stroke:#1565c0
    style LIME fill:#c8e6c9,stroke:#388e3c
    style SHAP_L fill:#c8e6c9,stroke:#388e3c
    style SHAP_G fill:#c8e6c9,stroke:#388e3c
    style RF fill:#c8e6c9,stroke:#388e3c
    style IML fill:#c8e6c9,stroke:#388e3c
```

---

## XAI-Techniken √úbersicht

| XAI-Technik | Beschreibung | Bibliotheken |
|-------------|--------------|--------------|
| **LIME** | Lokale Erkl√§rungen durch interpretierbare Surrogate-Modelle | lime, Skater |
| **SHAP** | Berechnet Feature-Beitr√§ge basierend auf Spieltheorie | shap, Dalex |
| **Break Down** | Aufschl√ºsselung des Vorhersagebeitrags jeder Variable | Dalex |
| **Permutation Importance** | Ermittelt Wichtigkeit durch Feature-Permutation | ELI5, Skater |
| **Partial Dependence** | Zeigt Abh√§ngigkeit der Vorhersage von einem Feature | Skater, Dalex |
| **Counterfactuals** | Findet alternative Eingaben zur Erkl√§rung | Alibi-Explain |
| **Anchors** | Entdeckt Regeln, die die Vorhersage erkl√§ren | Alibi-Explain |

---

## Counterfactual Explanations

### Konzept

**Counterfactual Explanations** (kontrafaktische Erkl√§rungen) beantworten die Frage: *"Was m√ºsste anders sein, damit das Modell eine andere Entscheidung trifft?"*

```mermaid
flowchart LR
    subgraph Faktisch["üìä Faktische Situation"]
        F["Kreditantrag: Abgelehnt<br/>Einkommen: 35.000‚Ç¨<br/>Schulden: 15.000‚Ç¨<br/>Besch√§ftigung: 2 Jahre"]
    end
    
    subgraph Kontrafaktisch["‚úÖ Counterfactual"]
        CF["Kreditantrag: Genehmigt<br/>Einkommen: 35.000‚Ç¨<br/>Schulden: 8.000‚Ç¨<br/>Besch√§ftigung: 2 Jahre"]
    end
    
    F -->|"Minimale<br/>√Ñnderung"| CF
    
    style F fill:#ffcccc,stroke:#cc0000
    style CF fill:#ccffcc,stroke:#00cc00
```

### Eigenschaften guter Counterfactuals

| Eigenschaft | Beschreibung |
|-------------|--------------|
| **Minimal** | So wenig √Ñnderungen wie m√∂glich |
| **Plausibel** | Die √Ñnderungen sind realistisch umsetzbar |
| **Actionable** | Der Betroffene kann die √Ñnderungen beeinflussen |
| **Divers** | Mehrere alternative Wege zum Ziel aufzeigen |

### Anwendungsbeispiel

Das folgende Beispiel zeigt die grundlegende Verwendung. In der Praxis erfordert die Bibliothek weitere Konfiguration.

```python
from alibi.explainers import CounterFactual

# Counterfactual Explainer erstellen
cf = CounterFactual(model.predict_proba, shape=(1, n_features))

# Counterfactual f√ºr abgelehnten Kreditantrag finden
explanation = cf.explain(abgelehnter_antrag)

# Ergebnis zeigt minimale √Ñnderungen f√ºr andere Entscheidung
# z.B.: "Reduzieren Sie Ihre Schulden um 7.000‚Ç¨ f√ºr eine Genehmigung"
```

**Vorteil:** Counterfactuals sind intuitiv verst√§ndlich und geben konkrete Handlungsempfehlungen.

---

## Anchors

### Konzept

**Anchors** sind Regeln, die eine Vorhersage "verankern" ‚Äì sie beschreiben die *hinreichenden Bedingungen*, unter denen das Modell mit hoher Wahrscheinlichkeit dieselbe Entscheidung trifft.

```mermaid
flowchart TD
    subgraph Anchor["‚öì Anchor-Regel"]
        RULE["WENN Geschlecht = weiblich<br/>UND Klasse ‚â§ 2<br/>DANN √úberlebt = Ja<br/>(Precision: 97%)"]
    end
    
    subgraph Anwendung["üìä Anwendung"]
        P1["Rose: weiblich, 1. Klasse ‚Üí ‚úÖ"]
        P2["Mary: weiblich, 2. Klasse ‚Üí ‚úÖ"]
        P3["Jack: m√§nnlich, 3. Klasse ‚Üí ‚ùì"]
    end
    
    Anchor --> Anwendung
    
    style Anchor fill:#e3f2fd,stroke:#1565c0
    style P1 fill:#c8e6c9,stroke:#388e3c
    style P2 fill:#c8e6c9,stroke:#388e3c
    style P3 fill:#ffcccc,stroke:#cc0000
```

### Vergleich der Erkl√§rungsarten

Anchors liefern einen anderen Erkl√§rungstyp als andere XAI-Methoden. W√§hrend LIME (siehe Abschnitt oben) numerische Gewichte liefert, die zeigen *wie stark* ein Feature wirkt, geben Anchors klare Regeln an, *wann* eine Vorhersage gilt.

| Aspekt | Gewicht-basiert (z.B. LIME) | Regel-basiert (Anchors) |
|--------|----------------------------|-------------------------|
| **Output** | "Alter hat Gewicht +0.3" | "WENN Alter < 30 DANN ..." |
| **Interpretation** | Erfordert Verst√§ndnis von Gewichten | Lesbar wie Gesch√§ftsregel |
| **Antwort auf** | "Wie stark wirkt jedes Feature?" | "Unter welchen Bedingungen gilt diese Vorhersage?" |
| **Besonders geeignet f√ºr** | Technische Analyse | Kommunikation an Laien |

### Code-Beispiel

```python
from alibi.explainers import AnchorTabular

# Anchor Explainer erstellen
anchor_exp = AnchorTabular(
    predictor=model.predict,
    feature_names=feature_names
)
anchor_exp.fit(X_train)

# Anchor f√ºr einzelne Instanz
explanation = anchor_exp.explain(rose.values)

# Ausgabe: "IF sex = female AND pclass <= 2 THEN survived = 1"
print(f"Anchor: {explanation.anchor}")
print(f"Precision: {explanation.precision:.2%}")
```

---

## Ceteris Paribus Analysen

### Konzept

Ceteris Paribus ("unter sonst gleichen Bedingungen") Analysen zeigen, wie sich die Vorhersage √§ndert, wenn nur ein Feature variiert wird, w√§hrend alle anderen konstant bleiben.

### Anwendungsbeispiel

```python
# Was w√§re wenn: Jack in verschiedenen Passagierklassen?
jack_cp = jack.copy()

for pclass in [1, 2, 3]:
    jack_cp['pclass'] = pclass
    pred = model.predict_proba(jack_cp)[0][1] * 100
    print(f"Jack in {pclass}. Klasse: {pred:.1f}% √úberlebenschance")
```

### Erkenntnisse aus dem Titanic-Beispiel

- **Alter**: J√ºngere Personen hatten tendenziell h√∂here √úberlebenschancen ("Women and children first")
- **Passagierklasse**: 1. Klasse hatte deutlich h√∂here √úberlebenschancen
- **Geschlecht dominiert**: Selbst ein Mann in 1. Klasse h√§tte schlechtere Chancen als eine Frau in 3. Klasse

---

## Best Practices

### Empfehlungen f√ºr den Einsatz

1. **Kombiniere lokale und globale Erkl√§rungen**: Nutze SHAP Summary f√ºr den √úberblick und Waterfall Plots f√ºr Einzelf√§lle

2. **Validiere Erkl√§rungen**: Pr√ºfe, ob die Erkl√§rungen mit Dom√§nenwissen √ºbereinstimmen

3. **Ber√ºcksichtige Stakeholder**: W√§hle die Visualisierung passend zur Zielgruppe

4. **Dokumentiere Limitationen**: XAI-Methoden sind selbst Approximationen

### Wann welche Methode?

| Situation | Empfohlene Methode |
|-----------|-------------------|
| Schnelle Feature-√úbersicht | RF Importance |
| Einzelne Kundenentscheidung erkl√§ren | LIME oder SHAP Waterfall |
| Regulatorische Anforderungen | SHAP (theoretisch fundiert) |
| Interaktive Exploration | InterpretML Dashboard |
| Minimal Setup | ELI5 |

---

## Weiterf√ºhrende Ressourcen

### Dokumentation

- **LIME**: [github.com/marcotcr/lime](https://github.com/marcotcr/lime)
- **SHAP**: [shap.readthedocs.io](https://shap.readthedocs.io/)
- **ELI5**: [eli5.readthedocs.io](https://eli5.readthedocs.io/)
- **InterpretML**: [interpret.ml](https://interpret.ml/)

### Wissenschaftliche Paper

- **LIME**: "Why Should I Trust You?" (Ribeiro et al., 2016)
- **SHAP**: "A Unified Approach to Interpreting Model Predictions" (Lundberg & Lee, 2017)

### Video-Tutorials

- [StatQuest: SHAP Values Explained](https://www.youtube.com/watch?v=VB9uV-x0fGE)
- [KNIME: Explainable AI](https://www.youtube.com/watch?v=Xv5xQQe2a3w)

---

## Zusammenfassung

> **Kernpunkte:**
> - XAI macht ML-Modelle verst√§ndlich und erh√∂ht das Vertrauen
> - **SHAP** ist die theoretisch fundierteste Methode f√ºr lokale und globale Erkl√§rungen
> - **LIME** eignet sich hervorragend f√ºr schnelle lokale Erkl√§rungen
> - **ELI5** bietet den einfachsten Einstieg
> - Die Wahl des Frameworks h√§ngt von Anwendungsfall und Zielgruppe ab
> - Kombiniere verschiedene Methoden f√ºr ein vollst√§ndiges Bild

---

**Version:** 1.2     
**Stand:** Januar 2026     
**Kurs:** Machine Learning. Verstehen. Anwenden. Gestalten.     
