---
layout: default
title: Methoden & Frameworks
parent: XAI
grand_parent: Konzepte
nav_order: 1
description: "EinfÃ¼hrung in Explainable AI (XAI): Methoden und Frameworks zur ErklÃ¤rbarkeit von ML-Modellen mit LIME, SHAP, ELI5 und InterpretML"
has_toc: true
---

# Methoden & Frameworks
{: .no_toc }

> **Explainable AI (XAI) umfasst Methoden und Techniken, die ML-Modelle fÃ¼r Menschen verstÃ¤ndlich und nachvollziehbar machen.** 

---

## Inhaltsverzeichnis
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## EinfÃ¼hrung in XAI

### Was ist Explainable AI?

Explainable AI (XAI) ist ein Ansatz der kÃ¼nstlichen Intelligenz, der darauf abzielt, dass die Funktionsweise und Entscheidungen von ML-Modellen fÃ¼r Menschen verstÃ¤ndlich und nachvollziehbar sind.

```mermaid
flowchart TD
    subgraph Problem["ğŸ”² Das Black-Box-Problem"]
        INPUT[/"Eingabedaten"/]
        MODEL[("ML-Modell<br/>(Black Box)")]
        OUTPUT[/"Vorhersage"/]
        INPUT --> MODEL --> OUTPUT
    end
    
    subgraph LÃ¶sung["âœ… XAI-LÃ¶sung"]
        INPUT2[/"Eingabedaten"/]
        MODEL2[("ML-Modell")]
        OUTPUT2[/"Vorhersage"/]
        EXPLAIN["ğŸ“Š ErklÃ¤rung:<br/>Welche Features waren wichtig?<br/>Warum diese Entscheidung?"]
        INPUT2 --> MODEL2 --> OUTPUT2
        MODEL2 -.-> EXPLAIN
    end
    
    Problem --> LÃ¶sung
    
    style Problem fill:#ffcccc,stroke:#cc0000
    style LÃ¶sung fill:#ccffcc,stroke:#00cc00
    style EXPLAIN fill:#ffffcc,stroke:#cccc00
```

### Warum ist XAI wichtig?

Die Umsetzung von XAI-Methoden trÃ¤gt dazu bei, das Vertrauen in KI-Systeme zu erhÃ¶hen, indem sie Transparenz und Nachvollziehbarkeit in den Entscheidungsprozess bringen.

| Bereich | Bedeutung von XAI |
|---------|-------------------|
| **Medizin** | Ã„rzte mÃ¼ssen verstehen, warum ein Modell eine Diagnose vorschlÃ¤gt |
| **Finanzwesen** | Kreditentscheidungen mÃ¼ssen gegenÃ¼ber Kunden begrÃ¼ndbar sein |
| **Rechtswesen** | Algorithmen mÃ¼ssen den Anforderungen an Fairness und Nachvollziehbarkeit genÃ¼gen |
| **Compliance** | DSGVO und andere Regularien fordern ErklÃ¤rbarkeit automatisierter Entscheidungen |

---

## XAI-AnsÃ¤tze im Ãœberblick

```mermaid
flowchart TD
    XAI["ğŸ” XAI-AnsÃ¤tze"]
    
    XAI --> IM["ğŸ“ Interpretable Models"]
    XAI --> LE["ğŸ¯ Local Explanation"]
    XAI --> GE["ğŸŒ Global Explanation"]
    
    IM --> IM1["Decision Trees"]
    IM --> IM2["Lineare Regression"]
    IM --> IM3["Regelbasierte Systeme"]
    
    LE --> LE1["LIME"]
    LE --> LE2["SHAP (lokal)"]
    LE --> LE3["Break-Down Analyse"]
    
    GE --> GE1["Feature Importance"]
    GE --> GE2["SHAP Summary"]
    GE --> GE3["Partial Dependence"]
    
    style XAI fill:#e1f5fe,stroke:#01579b
    style IM fill:#fff3e0,stroke:#e65100
    style LE fill:#e8f5e9,stroke:#2e7d32
    style GE fill:#fce4ec,stroke:#c2185b
```

### Interpretable Models

Verwendung von ML-Modellen, die von Grund auf so konzipiert sind, dass sie erklÃ¤rbar sind:

- **Decision Trees**: Klare Entscheidungsregeln, visuell darstellbar
- **Lineare Regression**: Koeffizienten zeigen direkt den Einfluss jedes Features
- **Regelbasierte Systeme**: Explizite IF-THEN-Regeln

### Local Explanation

ErklÃ¤rung individueller Vorhersagen durch Analyse der wichtigsten Features und ihrer AusprÃ¤gungen:

> **Beispiel**: Warum wurde fÃ¼r Passagier X vorhergesagt, dass er Ã¼berlebt?
> - Geschlecht: weiblich â†’ +45% Ãœberlebenschance
> - Klasse: 1. Klasse â†’ +20% Ãœberlebenschance
> - Alter: 22 Jahre â†’ +5% Ãœberlebenschance

### Global Explanation

Ganzheitliche ErklÃ¤rung der PrognosefÃ¤higkeit eines ML-Modells:

- **Feature Importance**: Welche Merkmale sind insgesamt am wichtigsten?
- **Partial Dependence**: Wie beeinflusst ein Feature die Vorhersage Ã¼ber alle Datenpunkte?
- **Accumulated Local Dependence**: Robustere Alternative zu Partial Dependence

---

## SHAP â€“ SHapley Additive exPlanations

### Konzept

SHAP basiert auf der Spieltheorie und dem Shapley-Wert, der ursprÃ¼nglich zur fairen Verteilung von Gewinnen in Koalitionen entwickelt wurde.

```mermaid
flowchart LR
    subgraph Spieltheorie["ğŸ² Spieltheorie-Analogie"]
        P1["Spieler A"]
        P2["Spieler B"]
        P3["Spieler C"]
        GEWINN["ğŸ’° Gewinn"]
        P1 & P2 & P3 --> GEWINN
    end
    
    subgraph ML["ğŸ¤– ML-Kontext"]
        F1["Feature 1<br/>(Alter)"]
        F2["Feature 2<br/>(Geschlecht)"]
        F3["Feature 3<br/>(Klasse)"]
        PRED["ğŸ“Š Vorhersage"]
        F1 & F2 & F3 --> PRED
    end
    
    Spieltheorie -.->|"Ãœbertragung"| ML
    
    style Spieltheorie fill:#e3f2fd,stroke:#1565c0
    style ML fill:#f3e5f5,stroke:#7b1fa2
```

### Berechnung des Shapley-Werts

Der Shapley-Wert berÃ¼cksichtigt alle mÃ¶glichen Kombinationen von Features und berechnet den durchschnittlichen Beitrag jedes Features:

1. Betrachte alle mÃ¶glichen Teilmengen von Features
2. Berechne fÃ¼r jede Teilmenge die Vorhersage mit und ohne das Feature
3. Bilde den gewichteten Durchschnitt Ã¼ber alle Kombinationen

### SHAP-Visualisierungen

| Visualisierung      | Beschreibung                                  | Scope  |
| ------------------- | --------------------------------------------- | ------ |
| **Waterfall Plot**  | Zeigt schrittweise den Beitrag jedes Features | Lokal  |
| **Force Plot**      | Kompakte Darstellung der Feature-BeitrÃ¤ge     | Lokal  |
| **Summary Plot**    | Ãœbersicht Ã¼ber alle Features und Datenpunkte  | Global |
| **Dependence Plot** | Einfluss eines Features auf die Vorhersage    | Global |

### Code-Beispiel

```python
import shap

# SHAP Explainer erstellen
explainer = shap.TreeExplainer(model)

# SHAP-Werte berechnen
shap_values = explainer(data_test)

# Waterfall Plot fÃ¼r einzelne Vorhersage
shap.plots.waterfall(shap_values[0])

# Summary Plot fÃ¼r globale Ãœbersicht
shap.plots.summary(shap_values)
```

---

## LIME â€“ Local Interpretable Model-agnostic Explanations

### Konzept

LIME erklÃ¤rt einzelne Vorhersagen, indem es ein einfaches, interpretierbares Modell lokal um die zu erklÃ¤rende Instanz herum trainiert.

```mermaid
flowchart TD
    subgraph LIME["LIME-Prozess"]
        INST["ğŸ¯ Zu erklÃ¤rende Instanz"]
        PERT["ğŸ”„ Perturbierte Samples<br/>generieren"]
        WEIGHT["âš–ï¸ Gewichtung nach<br/>Ã„hnlichkeit"]
        LOCAL["ğŸ“ Lokales lineares<br/>Modell trainieren"]
        EXPLAIN["ğŸ“Š ErklÃ¤rung<br/>extrahieren"]
        
        INST --> PERT --> WEIGHT --> LOCAL --> EXPLAIN
    end
    
    style INST fill:#ffeb3b,stroke:#f57f17
    style EXPLAIN fill:#4caf50,stroke:#2e7d32
```

### Funktionsweise

1. **Sample-Generierung**: Erzeuge Ã¤hnliche Datenpunkte durch Perturbation
2. **Gewichtung**: Gewichte Samples nach NÃ¤he zur Original-Instanz
3. **Lokales Modell**: Trainiere ein interpretierbares Modell (z.B. lineare Regression)
4. **Interpretation**: Die Koeffizienten des lokalen Modells erklÃ¤ren die Vorhersage

### Code-Beispiel

```python
from lime.lime_tabular import LimeTabularExplainer

# LIME Explainer erstellen
explainer = LimeTabularExplainer(
    training_data=data_train.values,
    feature_names=data_train.columns.tolist(),
    class_names=['Nicht Ã¼berlebt', 'Ãœberlebt'],
    mode='classification'
)

# ErklÃ¤rung fÃ¼r einzelne Instanz
explanation = explainer.explain_instance(
    data_row=rose.values[0],
    predict_fn=model.predict_proba,
    num_features=5
)

# Visualisierung
explanation.show_in_notebook()
```

---

## ELI5 â€“ Explain Like I'm 5

### Konzept

ELI5 ist ein Framework, das ErklÃ¤rungen so einfach wie mÃ¶glich darstellt â€“ wie fÃ¼r ein 5-jÃ¤hriges Kind. Es fokussiert auf Permutation Importance.

### Permutation Importance

Die Methode misst die Wichtigkeit eines Features, indem sie dessen Werte zufÃ¤llig permutiert und den Einfluss auf die Modellleistung beobachtet:

```mermaid
flowchart LR
    subgraph Original["ğŸ“Š Original"]
        DATA1["Daten"]
        SCORE1["Score: 0.85"]
        DATA1 --> SCORE1
    end
    
    subgraph Permutiert["ğŸ”€ Feature X permutiert"]
        DATA2["Daten<br/>(X gemischt)"]
        SCORE2["Score: 0.65"]
        DATA2 --> SCORE2
    end
    
    Original --> Permutiert
    Permutiert --> IMP["ğŸ“ˆ Importance(X) = 0.85 - 0.65 = 0.20"]
    
    style IMP fill:#c8e6c9,stroke:#388e3c
```

### Code-Beispiel

```python
import eli5
from eli5.sklearn import PermutationImportance

# Permutation Importance berechnen
perm = PermutationImportance(model, random_state=42)
perm.fit(data_test, target_test)

# HTML-Darstellung
eli5.show_weights(perm, feature_names=data_test.columns.tolist())

# Feature-Gewichte des Modells
eli5.show_weights(model, feature_names=data_train.columns.tolist())
```

---

## InterpretML â€“ Microsoft Framework

### Konzept

InterpretML ist Microsofts umfassendes Open-Source-Framework fÃ¼r Explainable AI, das sowohl interpretierbare Modelle als auch Black-Box-ErklÃ¤rungen unterstÃ¼tzt.

### Kernfunktionen

| Funktion | Beschreibung |
|----------|--------------|
| **Explainable Boosting Machine (EBM)** | Interpretierbares Modell mit Boosting-Performance |
| **SHAP Kernel** | Black-Box-ErklÃ¤rungen fÃ¼r beliebige Modelle |
| **Interaktive Dashboards** | Web-basierte Visualisierungen |
| **Unified API** | Einheitliche Schnittstelle fÃ¼r verschiedene ErklÃ¤rungsmethoden |

### Code-Beispiel

```python
from interpret import show
from interpret.blackbox import ShapKernel

# SHAP Kernel Explainer
shap_explainer = ShapKernel(
    predict_fn=model.predict_proba,
    data=data_train,
    feature_names=data_train.columns.tolist()
)

# Lokale ErklÃ¤rung
local_explanation = shap_explainer.explain_local(
    X=rose,
    y=None,
    name="Rose"
)

# Interaktives Dashboard
show(local_explanation)
```

---

## Feature Importance (Random Forest)

### Konzept

Random Forest berechnet die Feature Importance basierend darauf, wie stark jedes Feature zur Reduktion der Unreinheit (Impurity) in den EntscheidungsbÃ¤umen beitrÃ¤gt.

### Vorteile

- **Schnell**: Direkt im Training berechnet, kein zusÃ¤tzlicher Aufwand
- **Integriert**: In scikit-learn bereits enthalten
- **Einfach interpretierbar**: Direkte Rangfolge der Features

### EinschrÃ¤nkungen

- Zeigt keine **Richtung** des Einflusses (positiv/negativ)
- Kann bei korrelierten Features irrefÃ¼hrend sein
- Nur fÃ¼r Tree-basierte Modelle verfÃ¼gbar

### Code-Beispiel

```python
import pandas as pd
import plotly.express as px

# Feature Importance extrahieren
feature_importance = pd.DataFrame({
    'Feature': data_train.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

# Visualisierung
fig = px.bar(
    feature_importance,
    x='Importance',
    y='Feature',
    orientation='h',
    title='Feature Importance',
    color='Importance',
    color_continuous_scale='viridis'
)
fig.show()
```

---

## Framework-Vergleich

### Ãœbersichtstabelle

| Framework | StÃ¤rken | SchwÃ¤chen | Einsteigerfreundlichkeit |
|-----------|---------|-----------|--------------------------|
| **LIME** | Sehr intuitiv, gute Visualisierung, schnell lokal | Nur lokale ErklÃ¤rungen, kann instabil sein | â­â­â­â­â­ |
| **SHAP** | Theoretisch fundiert, beste Visualisierungen, lokal & global | Kann langsam sein, komplexeres Konzept | â­â­â­â­ |
| **ELI5** | Extrem einfach, minimaler Code, schnell | Weniger Visualisierungen, weniger Features | â­â­â­â­â­ |
| **InterpretML** | Interaktive Dashboards, umfassend, professionell | Komplexer Setup, Overhead fÃ¼r einfache Aufgaben | â­â­â­ |
| **RF Importance** | Extrem schnell, in sklearn integriert | Nur Feature Importance, keine Richtung | â­â­â­â­â­ |

### Entscheidungshilfe

```mermaid
flowchart TD
    START["â“ Welches XAI-Framework?"]
    
    START --> Q1{"Einzelne Vorhersage<br/>oder gesamtes Modell?"}
    
    Q1 -->|"Einzelne<br/>Vorhersage"| Q2{"Schnelligkeit<br/>wichtig?"}
    Q1 -->|"Gesamtes<br/>Modell"| Q3{"Tree-basiertes<br/>Modell?"}
    
    Q2 -->|"Ja"| LIME["ğŸ” LIME"]
    Q2 -->|"Nein"| SHAP_L["ğŸ¯ SHAP"]
    
    Q3 -->|"Ja"| RF["ğŸŒ² RF Importance<br/>+ SHAP"]
    Q3 -->|"Nein"| Q4{"Interaktives<br/>Dashboard nÃ¶tig?"}
    
    Q4 -->|"Ja"| IML["ğŸ¢ InterpretML"]
    Q4 -->|"Nein"| SHAP_G["ğŸ¯ SHAP Summary"]
    
    style START fill:#e3f2fd,stroke:#1565c0
    style LIME fill:#c8e6c9,stroke:#388e3c
    style SHAP_L fill:#c8e6c9,stroke:#388e3c
    style SHAP_G fill:#c8e6c9,stroke:#388e3c
    style RF fill:#c8e6c9,stroke:#388e3c
    style IML fill:#c8e6c9,stroke:#388e3c
```

---

## XAI-Techniken Ãœbersicht

| XAI-Technik | Beschreibung | Bibliotheken |
|-------------|--------------|--------------|
| **LIME** | Lokale ErklÃ¤rungen durch interpretierbare Surrogate-Modelle | lime, Skater |
| **SHAP** | Berechnet Feature-BeitrÃ¤ge basierend auf Spieltheorie | shap, Dalex |
| **Break Down** | AufschlÃ¼sselung des Vorhersagebeitrags jeder Variable | Dalex |
| **Permutation Importance** | Ermittelt Wichtigkeit durch Feature-Permutation | ELI5, Skater |
| **Partial Dependence** | Zeigt AbhÃ¤ngigkeit der Vorhersage von einem Feature | Skater, Dalex |
| **Counterfactuals** | Findet alternative Eingaben zur ErklÃ¤rung | Alibi-Explain |
| **Anchors** | Entdeckt Regeln, die die Vorhersage erklÃ¤ren | Alibi-Explain |

---

## Ceteris Paribus Analysen

### Konzept

Ceteris Paribus ("unter sonst gleichen Bedingungen") Analysen zeigen, wie sich die Vorhersage Ã¤ndert, wenn nur ein Feature variiert wird, wÃ¤hrend alle anderen konstant bleiben.

### Anwendungsbeispiel

```python
# Was wÃ¤re wenn: Jack in verschiedenen Passagierklassen?
jack_cp = jack.copy()

for pclass in [1, 2, 3]:
    jack_cp['pclass'] = pclass
    pred = model.predict_proba(jack_cp)[0][1] * 100
    print(f"Jack in {pclass}. Klasse: {pred:.1f}% Ãœberlebenschance")
```

### Erkenntnisse aus dem Titanic-Beispiel

- **Alter**: JÃ¼ngere Personen hatten tendenziell hÃ¶here Ãœberlebenschancen ("Women and children first")
- **Passagierklasse**: 1. Klasse hatte deutlich hÃ¶here Ãœberlebenschancen
- **Geschlecht dominiert**: Selbst ein Mann in 1. Klasse hÃ¤tte schlechtere Chancen als eine Frau in 3. Klasse

---

## Best Practices

### Empfehlungen fÃ¼r den Einsatz

1. **Kombiniere lokale und globale ErklÃ¤rungen**: Nutze SHAP Summary fÃ¼r den Ãœberblick und Waterfall Plots fÃ¼r EinzelfÃ¤lle

2. **Validiere ErklÃ¤rungen**: PrÃ¼fe, ob die ErklÃ¤rungen mit DomÃ¤nenwissen Ã¼bereinstimmen

3. **BerÃ¼cksichtige Stakeholder**: WÃ¤hle die Visualisierung passend zur Zielgruppe

4. **Dokumentiere Limitationen**: XAI-Methoden sind selbst Approximationen

### Wann welche Methode?

| Situation | Empfohlene Methode |
|-----------|-------------------|
| Schnelle Feature-Ãœbersicht | RF Importance |
| Einzelne Kundenentscheidung erklÃ¤ren | LIME oder SHAP Waterfall |
| Regulatorische Anforderungen | SHAP (theoretisch fundiert) |
| Interaktive Exploration | InterpretML Dashboard |
| Minimal Setup | ELI5 |

---

## WeiterfÃ¼hrende Ressourcen

### Dokumentation

- **LIME**: [github.com/marcotcr/lime](https://github.com/marcotcr/lime)
- **SHAP**: [shap.readthedocs.io](https://shap.readthedocs.io/)
- **ELI5**: [eli5.readthedocs.io](https://eli5.readthedocs.io/)
- **InterpretML**: [interpret.ml](https://interpret.ml/)

### Wissenschaftliche Paper

- **LIME**: "Why Should I Trust You?" (Ribeiro et al., 2016)
- **SHAP**: "A Unified Approach to Interpreting Model Predictions" (Lundberg & Lee, 2017)

### Video-Tutorials

- [StatQuest: SHAP Values Explained](https://www.youtube.com/watch?v=VB9uV-x0fGE)
- [KNIME: Explainable AI](https://www.youtube.com/watch?v=Xv5xQQe2a3w)

---

## Zusammenfassung

> **Kernpunkte:**
> - XAI macht ML-Modelle verstÃ¤ndlich und erhÃ¶ht das Vertrauen
> - **SHAP** ist die theoretisch fundierteste Methode fÃ¼r lokale und globale ErklÃ¤rungen
> - **LIME** eignet sich hervorragend fÃ¼r schnelle lokale ErklÃ¤rungen
> - **ELI5** bietet den einfachsten Einstieg
> - Die Wahl des Frameworks hÃ¤ngt von Anwendungsfall und Zielgruppe ab
> - Kombiniere verschiedene Methoden fÃ¼r ein vollstÃ¤ndiges Bild

---

**Version:** 1.0     
**Stand:** Januar 2026     
**Kurs:** Machine Learning. Verstehen. Anwenden. Gestalten.     
