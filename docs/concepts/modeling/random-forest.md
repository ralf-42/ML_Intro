---
layout: default
title: Random Forest
parent: Modeling
grand_parent: Konzepte
nav_order: 3
description: "Random Forest kombiniert multiple Entscheidungsb√§ume zu einem robusten Ensemble-Modell f√ºr Klassifikation und Regression"
has_toc: true
---

# Random Forest
{: .no_toc }

> **Random Forest ist ein Ensemble-Algorithmus, der multiple Entscheidungsb√§ume kombiniert, um robuste und genaue Vorhersagen zu treffen. Durch Bagging und Feature-Randomisierung reduziert er Overfitting und liefert zuverl√§ssige Ergebnisse f√ºr Klassifikation und Regression.**

---

# Inhaltsverzeichnis
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Grundkonzept

Random Forest ist eine **Ensemble-Methode**, die mehrere Entscheidungsb√§ume zu einem leistungsf√§higen Gesamtmodell kombiniert. Der Name leitet sich aus der Kombination zweier Konzepte ab:

- **Random**: Zuf√§llige Auswahl von Daten und Features
- **Forest**: Sammlung (Wald) von Entscheidungsb√§umen

```mermaid
flowchart TD
    subgraph Trainingsdaten
        D[("Originaldatensatz")]
    end
    
    subgraph Bootstrap["Bootstrap Sampling"]
        B1["Stichprobe 1"]
        B2["Stichprobe 2"]
        B3["Stichprobe 3"]
        Bn["Stichprobe n"]
    end
    
    subgraph Forest["Random Forest"]
        T1["üå≤ Baum 1"]
        T2["üå≤ Baum 2"]
        T3["üå≤ Baum 3"]
        Tn["üå≤ Baum n"]
    end
    
    subgraph Aggregation
        V["Voting / Averaging"]
    end
    
    D --> B1
    D --> B2
    D --> B3
    D --> Bn
    
    B1 --> T1
    B2 --> T2
    B3 --> T3
    Bn --> Tn
    
    T1 --> V
    T2 --> V
    T3 --> V
    Tn --> V
    
    V --> P["Finale Vorhersage"]
    
    style D fill:#e1f5fe
    style V fill:#c8e6c9
    style P fill:#fff9c4
```

---

## Funktionsprinzip

### Bagging (Bootstrap Aggregating)

Random Forest nutzt das **Bagging-Prinzip**, bei dem jeder Baum auf einer zuf√§lligen Stichprobe der Trainingsdaten trainiert wird:

1. **Bootstrap-Sampling**: Ziehen von Datenpunkten mit Zur√ºcklegen
2. **Paralleles Training**: Jeder Baum wird unabh√§ngig trainiert
3. **Aggregation**: Kombination der Einzelvorhersagen

```mermaid
flowchart LR
    subgraph Original["<b>Originaldaten (N Samples)"]
        O1["Sample 1"]
        O2["Sample 2"]
        O3["Sample 3"]
        O4["Sample 4"]
        O5["Sample 5"]
    end
    
    subgraph Bootstrap1["<b>Bootstrap 1"]
        B1a["Sample 1"]
        B1b["Sample 3"]
        B1c["Sample 3"]
        B1d["Sample 5"]
        B1e["Sample 2"]
    end
    
    subgraph Bootstrap2["<b>Bootstrap 2"]
        B2a["Sample 2"]
        B2b["Sample 4"]
        B2c["Sample 1"]
        B2d["Sample 4"]
        B2e["Sample 5"]
    end
    
    Original -->|"Ziehen mit Zur√ºcklegen"| Bootstrap1
    Original -->|"Ziehen mit Zur√ºcklegen"| Bootstrap2
    
    style Original fill:#e3f2fd
    style Bootstrap1 fill:#fff3e0
    style Bootstrap2 fill:#f3e5f5
```


**Beispiel**:

Bei 3000 verf√ºgbaren Datens√§tzen zieht jeder Baum im Random Forest ein Bootstrap-Sample der Gr√∂√üe 3000, wobei mit Zur√ºcklegen gesampelt wird. Im Mittel gehen dabei etwa 2000 unterschiedliche Datens√§tze in das Training eines Baums ein, w√§hrend rund 1000 Datens√§tze f√ºr diesen Baum _Out-of-the-Bag_ bleiben.

Wird der Out-of-Bag-Score (siehe unten) aktiviert, werden diese nicht gezogenen Datens√§tze zur internen Validierung verwendet. Ist der Out-of-Bag-Score deaktiviert, werden zwar weiterhin dieselben Bootstrap-Samples erzeugt, die Out-of-Bag-Datens√§tze jedoch nicht zur Fehlersch√§tzung herangezogen.

### Feature-Randomisierung

An jedem Splitpunkt wird nur eine **zuf√§llige Teilmenge der Features** betrachtet:

| Parameter | Typischer Wert | Beschreibung |
|-----------|----------------|--------------|
| Klassifikation | ‚àöm Features | Wurzel aus Gesamtzahl der Features |
| Regression | m/3 Features | Ein Drittel der Features |
| max_features | 'sqrt', 'log2', int | Scikit-learn Parameter |

Diese Randomisierung f√ºhrt zu **dekorrellierten B√§umen**, was die Varianz des Ensembles reduziert.

---

## Vorhersage-Aggregation

```mermaid
flowchart TD
    subgraph Input["<b>Neuer Datenpunkt"]
        X["Features: x‚ÇÅ, x‚ÇÇ, ..., x‚Çô"]
    end
    
    subgraph Trees["<b>Individuelle Vorhersagen"]
        T1["Baum 1: Klasse A"]
        T2["Baum 2: Klasse B"]
        T3["Baum 3: Klasse A"]
        T4["Baum 4: Klasse A"]
        T5["Baum 5: Klasse B"]
    end
    
    subgraph Klassifikation["<b>Klassifikation: Majority Voting"]
        MV["A: 3 Stimmen ‚úì<br/>B: 2 Stimmen"]
    end
    
    subgraph Regression["<b>Regression: Mittelwert"]
        AVG["(y‚ÇÅ + y‚ÇÇ + y‚ÇÉ + y‚ÇÑ + y‚ÇÖ) / 5"]
    end
    
    X --> T1
    X --> T2
    X --> T3
    X --> T4
    X --> T5
    
    T1 --> MV
    T2 --> MV
    T3 --> MV
    T4 --> MV
    T5 --> MV
    
    MV --> Result1["Vorhersage: Klasse A"]
    AVG --> Result2["Vorhersage: »≥"]
    
    style Input fill:#e1f5fe
    style Result1 fill:#c8e6c9
    style Result2 fill:#c8e6c9
```

| Aufgabe | Aggregationsmethode | Beschreibung |
|---------|---------------------|--------------|
| **Klassifikation** | Majority Voting | H√§ufigste Klasse gewinnt |
| **Regression** | Mittelwert/Median | Durchschnitt aller Vorhersagen |
| **Wahrscheinlichkeit** | Durchschnitt | Mittlere Wahrscheinlichkeit pro Klasse |

---


## Wichtige Hyperparameter

```mermaid
mindmap
  root((Random Forest<br/>Hyperparameter))
    Baumstruktur
      n_estimators
      max_depth
      min_samples_split
      min_samples_leaf
    Feature-Auswahl
      max_features
      bootstrap
      max_samples
    Performance
      n_jobs
      random_state
      warm_start
    Regularisierung
      max_leaf_nodes
      min_impurity_decrease
      ccp_alpha
```

### Parameter-√úbersicht

| Parameter | Default | Empfohlener Bereich | Effekt |
|-----------|---------|---------------------|--------|
| `n_estimators` | 100 | 100-500 | Mehr B√§ume ‚Üí stabilere Vorhersagen |
| `max_depth` | None | 5-30 | Begrenzt Komplexit√§t, verhindert Overfitting |
| `min_samples_split` | 2 | 2-20 | H√∂her ‚Üí konservativere Splits |
| `min_samples_leaf` | 1 | 1-10 | Mindestgr√∂√üe der Blattknoten |
| `max_features` | 'sqrt' | 'sqrt', 'log2', 0.3-0.7 | Dekorrelation der B√§ume |
| `bootstrap` | True | True/False | Bootstrap Sampling aktivieren |

---

## Feature Importance

Random Forest bietet eingebaute **Feature Importance** basierend auf der durchschnittlichen Reduktion der Unreinheit (Gini/Entropy):

```mermaid
xychart-beta
    title "Feature Importance (Beispiel Iris Dataset)"
    x-axis ["petal length", "petal width", "sepal length", "sepal width"]
    y-axis "Importance" 0 --> 0.5
    bar [0.44, 0.42, 0.09, 0.05]
```

---

## Out-of-Bag (OOB) Score

Durch **Bootstrap-Sampling** werden ca. **37% der Daten**[1] pro Baum nicht f√ºr ein Training verwendet. Diese k√∂nnen zur Validierung genutzt werden:

```python
model = RandomForestClassifier(
    n_estimators=100,
    oob_score=True,        # OOB Score aktivieren
    random_state=42,
    n_jobs=-1
)

model.fit(data_train, target_train)

print(f"OOB Score: {model.oob_score_:.4f}")
print(f"Test Score: {model.score(data_test, target_test):.4f}")
```

> **Vorteil**: OOB Score liefert eine Sch√§tzung der Generalisierungsf√§higkeit ohne zus√§tzlichen Validierungsdatensatz.

---

### Nutzung des OOB-Scores

Der **OOB-Score** dient beim Random Forest als interne Validierungsmetrik. Da er auf den Daten berechnet wird, die w√§hrend des Bootstrappings **nicht** f√ºr den Bau eines Baumes verwendet wurden, bietet er eine unverzerrte Sch√§tzung der Modellg√ºte.

Man nutzt ihn vor allem f√ºr:

1. **Validierung ohne Testset:** Er spart Daten, da man kein separates Validierungset abspalten muss.
    
2. **Effizientes Tuning:** Man kann Hyperparameter (wie die Tiefe der B√§ume) optimieren, ohne teure Cross-Validation-Schleifen durchzuf√ºhren.
    
3. **Konvergenz-Check:** Er zeigt an, ab wie vielen B√§umen das Modell ges√§ttigt ist und keine weitere Verbesserung mehr erzielt.
    

---

### Code-Beispiel: OOB-Fehler-Kurve

Dieses Skript zeigt, wie sich die Fehlerrate stabilisiert, je mehr B√§ume dem Wald hinzugef√ºgt werden.

Python

```Python
import pandas as pd
import plotly.express as px
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# 1. Daten vorbereiten (data, target)
data, target = make_classification(n_samples=1000, n_features=20, random_state=42)

# 2. Modell und Parameter
rf = RandomForestClassifier(oob_score=True, random_state=42, warm_start=True)
tree_range = range(10, 301, 10)
results = []

# 3. Iterative Berechnung
for n in tree_range:
    rf.set_params(n_estimators=n)
    rf.fit(data, target)
    oob_error = 1 - rf.oob_score_
    results.append({"n_estimators": n, "oob_error": oob_error})

# 4. DataFrame erstellen
df_results = pd.DataFrame(results)

# 5. Plotly Express Plot erstellen
fig = px.line(
    df_results, 
    x="n_estimators", 
    y="oob_error", 
    title="OOB-Fehlerrate vs. Anzahl der B√§ume",
    labels={"n_estimators": "Anzahl der B√§ume", "oob_error": "OOB-Fehler"},
    markers=True
)

# Layout-Anpassungen
fig.update_layout(
    xaxis_title="Anzahl der B√§ume (n_estimators)",
    yaxis_title="OOB-Fehlerrate",
    template="plotly_white"
)
```

**Was man im Plot suchen sollte:**

Sobald die Fehlerrate nur noch minimal schwankt (Plateau), hat man die optimale Anzahl an B√§umen erreicht. Mehr B√§ume erh√∂hen dann nur noch die Rechenlast, aber nicht mehr die Vorhersagekraft.


## Vor- und Nachteile

### Vorteile

| Vorteil | Erkl√§rung |
|---------|-----------|
| ‚úÖ **Robustheit** | Weniger anf√§llig f√ºr Overfitting als einzelne B√§ume |
| ‚úÖ **Keine Skalierung n√∂tig** | Funktioniert mit Original-Features |
| ‚úÖ **Gemischte Datentypen** | Verarbeitet numerische und kategoriale Features |
| ‚úÖ **Feature Importance** | Eingebaute Bewertung der Feature-Relevanz |
| ‚úÖ **Parallelisierbar** | B√§ume k√∂nnen parallel trainiert werden |
| ‚úÖ **OOB-Validierung** | Integrierte Cross-Validation |

### Nachteile

| Nachteil | Erkl√§rung |
|----------|-----------|
| ‚ùå **Black Box** | Schwieriger zu interpretieren als einzelner Baum |
| ‚ùå **Speicherbedarf** | Viele B√§ume ben√∂tigen viel Speicher |
| ‚ùå **Langsame Vorhersage** | Bei sehr vielen B√§umen |
| ‚ùå **Overfitting bei Rauschen** | Kann Rauschen in Daten √ºberanpassen |
| ‚ùå **Extrapolation** | Kann nicht √ºber Trainingsdatenbereich hinaus extrapolieren |

---

## Entscheidungshilfe: Wann Random Forest?

```mermaid
flowchart TD
    Start["Neues ML-Problem"] --> Q1{"Viele Features?"}
    
    Q1 -->|Ja| Q2{"Interpretierbarkeit<br/>wichtig?"}
    Q1 -->|Nein| Q3{"Kleine Datenmenge?"}
    
    Q2 -->|Sehr wichtig| DT["Decision Tree"]
    Q2 -->|Weniger wichtig| RF1["‚úÖ Random Forest"]
    
    Q3 -->|Ja| RF2["‚úÖ Random Forest<br/>(mit weniger B√§umen)"]
    Q3 -->|Nein| Q4{"H√∂chste Accuracy<br/>erforderlich?"}
    
    Q4 -->|Ja| XGB["XGBoost/LightGBM"]
    Q4 -->|Nein| RF3["‚úÖ Random Forest"]
    
    style RF1 fill:#c8e6c9
    style RF2 fill:#c8e6c9
    style RF3 fill:#c8e6c9
    style DT fill:#fff9c4
    style XGB fill:#e1f5fe
```

---

## Best Practices

### Do's ‚úÖ

| Empfehlung | Begr√ºndung |
|------------|------------|
| Mit 100 B√§umen starten | Guter Kompromiss zwischen Performance und Trainingszeit |
| OOB Score nutzen | Schnelle Validierung ohne separaten Split |
| Feature Importance analysieren | Hilft beim Verst√§ndnis und Feature Selection |
| n_jobs=-1 setzen | Nutzt alle CPU-Kerne f√ºr paralleles Training |
| Cross-Validation verwenden | Robustere Evaluation als einfacher Train-Test-Split |

### Don'ts ‚ùå

| Vermeiden | Grund |
|-----------|-------|
| Zu viele B√§ume ohne Verbesserung | Erh√∂ht nur Rechenaufwand |
| max_depth ignorieren | Kann zu sehr tiefen, √ºberangepassten B√§umen f√ºhren |
| Bei Zeitreihen ohne Vorsicht | Random Forest ignoriert zeitliche Ordnung |
| F√ºr Extrapolation verwenden | Kann nur innerhalb des Trainingsdatenbereichs vorhersagen |

---

## Zusammenfassung

```mermaid
mindmap
  root((Random Forest))
    Kernkonzepte
      Ensemble aus Decision Trees
      Bootstrap Aggregating
      Feature Randomisierung
      Majority Voting / Averaging
    St√§rken
      Robust gegen Overfitting
      Keine Feature-Skalierung
      Eingebaute Feature Importance
      Parallelisierbar
    Parameter
      n_estimators
      max_depth
      max_features
      min_samples_split
    Anwendung
      Klassifikation
      Regression
      Feature Selection
```

> **Kernaussage**: Random Forest kombiniert die Einfachheit von Entscheidungsb√§umen mit der Robustheit von Ensemble-Methoden. Durch Bagging und Feature-Randomisierung entstehen dekorrelierte B√§ume, deren aggregierte Vorhersagen stabiler und genauer sind als die eines einzelnen Baums.

---
[1] Mathematisch gesehen liegt die Wahrscheinlichkeit, dass ein spezifischer Datensatz bei einer Stichprobengr√∂√üe von $n$ nicht ausgew√§hlt wird, bei:

$$\left(1 - \frac{1}{n}\right)^n$$

F√ºr gro√üe $n$ n√§hert sich dieser Wert $1/e \approx 0,368$ an.

---


**Version:** 1.0     
**Stand:** Januar 2026     
**Kurs:** Machine Learning. Verstehen. Anwenden. Gestalten.    