---
layout: default
title: Isolation Forest
parent: Modeling
grand_parent: Konzepte
nav_order: 5
description: "Identifikation untypischer Datenpunkte durch Anomalie-Scores und Isolation Forest"
has_toc: true
---

# Anomalie-Erkennung
{: .no_toc }

> **Anomalie-Erkennung identifiziert Datenpunkte, die signifikant vom erwarteten Muster abweichen. Der Anomalie-Score quantifiziert dabei, wie untypisch ein Datenpunkt im Vergleich zum Rest des Datensatzes ist.**

---

# Inhaltsverzeichnis
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Was ist Anomalie-Erkennung?

Anomalie-Erkennung ist ein Verfahren des un√ºberwachten Lernens, das Datens√§tze identifiziert, die f√ºr die gesamte Datenbasis untypisch sind. Anomalien ‚Äì auch als Ausrei√üer oder Outliers bezeichnet ‚Äì weichen signifikant vom normalen Verhalten ab.

<img src="https://raw.githubusercontent.com/ralf-42/ML_Intro/main/07_image/isolation_forest.png" class="logo" width="750"/>



---

## Anomalie-Typen

Anomalien lassen sich in drei grundlegende Kategorien einteilen:

### Punkt-/Globale Anomalien

Ein einzelner Datenpunkt, der in Bezug auf den gesamten Datensatz als anomal zu klassifizieren ist.

**Beispiel:** Eine einzelne Transaktion von 50.000 ‚Ç¨ bei einem Kunden mit durchschnittlichen Transaktionen von 100 ‚Ç¨.

### Kontextuelle Anomalien

Ein Datenpunkt, der nur in einem bestimmten Kontext anomal erscheint.

**Beispiel:** Eine Au√üentemperatur von +30¬∞C im Dezember in Deutschland ist anomal, im Juli jedoch normal.

### Kollektive Anomalien

Eine Menge verwandter Datenpunkte ist gemeinsam anomal, obwohl einzelne Punkte normal erscheinen k√∂nnen.

**Beispiel:** Kreditkartendaten zeigen K√§ufe in den USA und Frankreich zur gleichen Zeit ‚Äì einzeln normal, zusammen verd√§chtig.

```mermaid
flowchart TD
    A[Anomalie-Typen] --> P[Punkt-Anomalie]
    A --> K[Kontextuelle Anomalie]
    A --> C[Kollektive Anomalie]
    
    P --> PE["Einzelner extremer Wert<br>z.B. ungew√∂hnlich hohe Transaktion"]
    K --> KE["Kontext-abh√§ngig anomal<br>z.B. 30¬∞C im Winter"]
    C --> CE["Gruppe gemeinsam anomal<br>z.B. gleichzeitige K√§ufe in verschiedenen L√§ndern"]
    
    style P fill:#e7f5ff,stroke:#1971c2
    style K fill:#fff3bf,stroke:#f59f00
    style C fill:#ffe3e3,stroke:#fa5252
```

---
## Isolation Forest

Isolation Forest ist der am h√§ufigsten verwendete Algorithmus zur Anomalie-Erkennung. Er basiert auf der Idee, dass Anomalien leichter zu isolieren sind als normale Datenpunkte.

### Funktionsweise

Der Algorithmus arbeitet mit einem Ensemble von Entscheidungsb√§umen (√§hnlich wie Random Forest):

1. **Zuf√§llige Partitionierung:** W√§hle zuf√§llig ein Feature und einen Splitwert
2. **Rekursive Aufteilung:** Teile die Daten bis zur Isolation einzelner Punkte
3. **Pfadl√§nge messen:** Anomalien ben√∂tigen weniger Splits zur Isolation
4. **Score berechnen:** Durchschnittliche Pfadl√§nge √ºber alle B√§ume

```mermaid
flowchart TD
    D[Datensatz] --> T1[Baum 1]
    D --> T2[Baum 2]
    D --> T3[...]
    D --> TN[Baum n]
    
    T1 --> |"Pfadl√§nge"| A[Aggregation]
    T2 --> |"Pfadl√§nge"| A
    T3 --> |"Pfadl√§nge"| A
    TN --> |"Pfadl√§nge"| A
    
    A --> S[Anomalie-Score]
    
    S --> |"Score ‚âà -1"| AN[üî¥ Anomalie]
    S --> |"Score ‚âà +1"| NO[üü¢ Normal]
    
    style AN fill:#ff6b6b,stroke:#c92a2a,color:#fff
    style NO fill:#51cf66,stroke:#2f9e44,color:#fff
```

### Grundannahmen

Der Isolation Forest basiert auf zwei zentralen Annahmen:

1. **Minderheitsklasse:** Anomalien sind im Vergleich zu normalen Daten selten
2. **Leichte Isolierbarkeit:** Anomalien haben ungew√∂hnliche Merkmalswerte und werden daher mit weniger Splits isoliert

---

## Praktische Implementierung

### Grundlegende Anwendung

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt

# Beispieldaten generieren
np.random.seed(42)
# Normale Daten: Cluster um (0, 0)
X_normal = np.random.randn(200, 2)
# Anomalien: verstreute Punkte
X_anomalies = np.random.uniform(low=-4, high=4, size=(20, 2))
X = np.vstack([X_normal, X_anomalies])

# Isolation Forest erstellen und trainieren
iso_forest = IsolationForest(
    n_estimators=100,      # Anzahl der B√§ume
    contamination=0.1,     # Erwarteter Anteil Anomalien
    random_state=42
)

# Vorhersage: 1 = normal, -1 = Anomalie
predictions = iso_forest.fit_predict(X)

# Anomalie-Scores abrufen (negativ = anomaler)
scores = iso_forest.decision_function(X)

print(f"Erkannte Anomalien: {(predictions == -1).sum()}")
print(f"Score-Bereich: {scores.min():.3f} bis {scores.max():.3f}")
```

### Wichtige Hyperparameter

| Parameter | Beschreibung | Typische Werte |
|:----------|:-------------|:---------------|
| `n_estimators` | Anzahl der B√§ume im Ensemble | 100-200 |
| `contamination` | Erwarteter Anteil Anomalien | 0.01-0.1 (1%-10%) |
| `max_samples` | Stichprobengr√∂√üe pro Baum | 'auto' oder Anzahl |
| `max_features` | Features pro Baum | 1.0 (alle) |

### Visualisierung der Ergebnisse

```python
import matplotlib.pyplot as plt

# Visualisierung
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Links: Klassifikation
colors = ['#51cf66' if p == 1 else '#ff6b6b' for p in predictions]
axes[0].scatter(X[:, 0], X[:, 1], c=colors, alpha=0.7, edgecolors='white')
axes[0].set_title('Anomalie-Erkennung')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')

# Legende hinzuf√ºgen
from matplotlib.patches import Patch
legend_elements = [
    Patch(facecolor='#51cf66', label='Normal'),
    Patch(facecolor='#ff6b6b', label='Anomalie')
]
axes[0].legend(handles=legend_elements)

# Rechts: Anomalie-Scores
scatter = axes[1].scatter(X[:, 0], X[:, 1], c=scores, cmap='RdYlGn', alpha=0.7)
axes[1].set_title('Anomalie-Scores')
axes[1].set_xlabel('Feature 1')
axes[1].set_ylabel('Feature 2')
plt.colorbar(scatter, ax=axes[1], label='Score (niedriger = anomaler)')

plt.tight_layout()
plt.show()
```


---

## Anwendungsgebiete

Die Anomalie-Erkennung findet in vielen Bereichen praktische Anwendung:

| Bereich | Anwendung | Beispiel |
|:--------|:----------|:---------|
| **Finanzen** | Betrugserkennung | Ungew√∂hnliche Kreditkartentransaktionen |
| **IT-Sicherheit** | Intrusion Detection | Anomale Netzwerkaktivit√§ten |
| **Produktion** | Qualit√§tskontrolle | Defekte Produkte erkennen |
| **Medizin** | Diagnose-Unterst√ºtzung | Ungew√∂hnliche Messwerte |
| **IoT** | Sensor-√úberwachung | Fehlerhafte Sensordaten |

---

## Best Practices

### Empfehlungen f√ºr die Praxis

1. **Contamination sch√§tzen:** Nutze Dom√§nenwissen, um den erwarteten Anomalie-Anteil realistisch einzusch√§tzen

2. **Feature-Skalierung:** Standardisiere Features vor der Anwendung, besonders bei unterschiedlichen Wertebereichen

3. **Mehrere Algorithmen testen:** Vergleiche Isolation Forest mit anderen Methoden wie One-Class SVM oder Autoencoder

4. **Schwellenwert anpassen:** Der Standard-Schwellenwert ist nicht immer optimal ‚Äì experimentiere mit verschiedenen Werten

5. **Ergebnisse validieren:** Lass Dom√§nenexperten die erkannten Anomalien pr√ºfen

### H√§ufige Fallstricke

> **Vorsicht vor √úberinterpretation:** Nicht jede erkannte Anomalie ist problematisch. Manche "Anomalien" sind einfach seltene, aber valide Datenpunkte.

```mermaid
flowchart TD
    A[Anomalie erkannt] --> B{Dom√§nenexperte<br>pr√ºfen}
    B --> |"Tats√§chliche Anomalie"| C[Aktion einleiten]
    B --> |"Seltener, aber valider Fall"| D[In Modell aufnehmen]
    B --> |"Datenfehler"| E[Daten korrigieren]
    
    style A fill:#ff6b6b,stroke:#c92a2a,color:#fff
    style C fill:#ffd43b,stroke:#fab005,color:#000
    style D fill:#51cf66,stroke:#2f9e44,color:#fff
    style E fill:#748ffc,stroke:#4c6ef5,color:#fff
```

---

## Vergleich mit anderen Methoden

| Methode | Vorteile | Nachteile | Geeignet f√ºr |
|:--------|:---------|:----------|:-------------|
| **Isolation Forest** | Schnell, skalierbar, keine Annahmen √ºber Verteilung | Schwer interpretierbar | Hochdimensionale Daten |
| **One-Class SVM** | Gut bei klarer Normalverteilung | Rechenintensiv bei gro√üen Daten | Niedrigdimensionale Daten |
| **LOF** | Erkennt lokale Anomalien | Langsam bei gro√üen Datens√§tzen | Dichte-basierte Anomalien |
| **Autoencoder** | Lernt komplexe Muster | Ben√∂tigt viele Daten, Tuning-aufw√§ndig | Bild-/Sequenzdaten |

---

## Zusammenfassung

- **Anomalie-Erkennung** identifiziert untypische Datenpunkte im Vergleich zum Normalverhalten
- **Drei Anomalie-Typen:** Punkt-, kontextuelle und kollektive Anomalien
- **Anomalie-Score:** Quantifiziert die Abweichung (-1 = anomal, +1 = normal)
- **Isolation Forest:** Standard-Algorithmus basierend auf der leichten Isolierbarkeit von Anomalien
- **Wichtig:** Dom√§nenwissen f√ºr Contamination-Parameter und Ergebnisvalidierung nutzen


---

**Version:** 1.0    
**Stand:** Januar 2026    
**Kurs:** Machine Learning. Verstehen. Anwenden. Gestalten.    