{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["6lC7tXqvuFla","86MedcxHubtn","cBTx3DqqujhN","8XLy7QnpuqmB","lvBkJqW5uvjX","ekj9DgEXxjzf"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fd913b05"},"source":["<p><font size=\"6\" color='grey'> <b>\n","Machine Learning\n","</b></font> </br></p>\n","<p><font size=\"5\" color='grey'> <b>\n","Principle Component Analysis - Cancer\n","</b></font> </br></p>\n","\n","---\n"]},{"cell_type":"code","source":["#@title üîß Colab-Umgebung { display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/Python_Modules\n","from ml_lib.utilities import get_ipinfo\n","import sys\n","print()\n","print(f\"Python Version: {sys.version}\")\n","print()\n","get_ipinfo()"],"metadata":{"id":"r0gKz9kbd9UV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 0 | Install & Import\n","---"],"metadata":{"id":"6lC7tXqvuFla"}},{"cell_type":"code","source":["# Install"],"metadata":{"id":"ZsYz675MuFlb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import\n","import numpy as np\n","from pandas import read_csv, DataFrame, concat\n","\n","from sklearn.datasets import fetch_openml\n","from sklearn.decomposition import PCA\n","\n","import plotly.express as px"],"metadata":{"id":"Y_c62toMuFlb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Warnung ausstellen\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"sONkeHWyuFlb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1 | Understand\n","---"],"metadata":{"id":"86MedcxHubtn"}},{"cell_type":"code","source":["import pandas as pd\n","df = read_csv('https://raw.githubusercontent.com/ralf-42/ML_Intro/main/02%20data/breast_cancer_wisconsin.csv')"],"metadata":{"id":"kjPA-rHk42gq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = df.copy()\n","target = data.pop(\"Class\")"],"metadata":{"id":"eXwjh-3e42gr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2 | Prepare\n","---"],"metadata":{"id":"cBTx3DqqujhN"}},{"cell_type":"code","source":["# drop na\n","data = data.dropna()\n","target = target.loc[data.index]"],"metadata":{"id":"T4onESYwukT_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3 | Modeling\n","---"],"metadata":{"id":"8XLy7QnpuqmB"}},{"cell_type":"code","source":["n_components = data.shape[1]\n","model = PCA(n_components=n_components)"],"metadata":{"id":"relJbXEnvA1t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pca_np = model.fit_transform(data)\n","pca_df = DataFrame(pca_np)"],"metadata":{"id":"fRW3r-GJQwdQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4 | Evaluate\n","---"],"metadata":{"id":"lvBkJqW5uvjX"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Erkl√§rte Varianz\n","</font></p>"],"metadata":{"id":"K_cE65E6S52J"}},{"cell_type":"markdown","source":["Die erkl√§rte Varianz ist der Anteil der Gesamtvarianz im Datensatz, der durch eine bestimmte Hauptkomponente oder eine Menge von Hauptkomponenten erkl√§rt wird. Sie ist ein Ma√ü daf√ºr, wie gut die Hauptkomponenten die Daten repr√§sentieren.\n","\n","**PCA**\n","\n","PCA ist eine Technik zur Dimensionsreduktion, die verwendet wird, um die Dimensionalit√§t eines Datensatzes zu reduzieren, indem er in einen niedrigerdimensionalen Raum transformiert wird. Dies geschieht durch die Suche nach den Hauptkomponenten des Datensatzes, die die Richtungen der gr√∂√üten Varianz in den Daten darstellen. Die erste Hauptkomponente erkl√§rt den gr√∂√üten Teil der Varianz, die zweite Hauptkomponente den zweitgr√∂√üten Teil der Varianz und so weiter.\n","\n","**Erkl√§rte Varianz in PCA**\n","\n","Im Zusammenhang mit PCA wird die erkl√§rte Varianz verwendet, um zu bestimmen, wie viele Hauptkomponenten im niedrigerdimensionalen Raum beibehalten werden sollen. Ziel ist es, so viele Hauptkomponenten beizubehalten, dass ein gro√üer Teil der Gesamtvarianz erkl√§rt wird, ohne zu viele Dimensionen beizubehalten.\n","\n","Die erkl√§rte Varianz jeder Hauptkomponente kann berechnet werden, indem das Eigenwert der Hauptkomponente durch die Summe der Eigenwerte aller Hauptkomponenten dividiert wird. Die Gesamtvarianz, die durch eine Menge von Hauptkomponenten erkl√§rt wird, ist die Summe der erkl√§rten Varianzen jeder Hauptkomponente.\n","\n","**Beispiel**\n","\n","Angenommen, ein Datensatz hat 10 Dimensionen und die PCA wird verwendet, um ihn auf 2 Dimensionen zu reduzieren. Die erste Hauptkomponente erkl√§rt 70 % der Gesamtvarianz und die zweite Hauptkomponente erkl√§rt 20 % der Gesamtvarianz. Zusammen erkl√§ren diese beiden Hauptkomponenten 90 % der Gesamtvarianz. Dies bedeutet, dass der Datensatz in einem 2-dimensionalen Raum mit einem Verlust von nur 10 % der Gesamtvarianz repr√§sentiert werden kann.\n","\n","**Schlussfolgerung**\n","\n","Die erkl√§rte Varianz ist ein wichtiges Konzept in PCA. Sie wird verwendet, um zu bestimmen, wie viele Hauptkomponenten im niedrigerdimensionalen Raum beibehalten werden sollen. Ziel ist es, so viele Hauptkomponenten beizubehalten, dass ein gro√üer Teil der Gesamtvarianz erkl√§rt wird, ohne zu viele Dimensionen beizubehalten."],"metadata":{"id":"s_OvrgnORocM"}},{"cell_type":"code","source":["opt_point = 90.0  # erkl√§rte Varianz sollte >= 90% sein\n","explained_variance = [round(v, 3) for v in model.explained_variance_ratio_ * 100]\n","cum_expl_variance = np.cumsum(model.explained_variance_ratio_ * 100)\n","pc_greater = np.argmax(cum_expl_variance >= opt_point) + 1"],"metadata":{"id":"qnOgoC3OwK8j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe \"erkl√§rte Varianz\"\n","print(\"-\" * 40)\n","print(\"Erkl√§rte Varianz (%):\\n\", explained_variance)\n","\n","print(\"-\" * 40)\n","print(\"Kum. erkl. Varianz (%):\\n\", cum_expl_variance)\n","\n","print(\"-\" * 40)\n","print(f\"Optimaler Punkt: {pc_greater}\", \"\\n\")"],"metadata":{"id":"NKKUiKRrwK8k","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","# @markdown   <p><font size=\"4\" color='green'>  Plot-Funktion</font> </br></p>\n","def plot_explained_variance(data, cum_sum, opt_point, d):\n","    # Erstelle ein DataFrame f√ºr die Daten\n","    df = DataFrame(\n","        {\n","            \"Principal Component\": range(1, data.shape[1] + 1),\n","            \"Cumulative Explained Variance\": cum_sum,\n","        }\n","    )\n","\n","    # Erstelle das Liniendiagramm mit plotly-express\n","    fig = px.line(\n","        df,\n","        x=\"Principal Component\",\n","        y=\"Cumulative Explained Variance\",\n","        title=\"Explained Variance by Principal Components\",\n","        labels={\"Cumulative Explained Variance\": \"Cumulative Explained Variance\"},\n","        width=700,\n","        height=400,\n","        markers=True,\n","    )\n","\n","    # F√ºge den optimalen Punkt als Scatter hinzu\n","    fig.add_scatter(\n","        x=[d],\n","        y=[cum_sum[d - 1]],\n","        mode=\"markers\",\n","        marker=dict(color=\"red\", size=10),\n","        name=\"Opt. Point >\" + str(opt_point) + \"%\",\n","    )\n","\n","    # Zeige die Grafik an\n","    fig.show()"],"metadata":{"id":"8A88lorhle6-","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Grafik \"erkl√§rte Varianz\"\n","plot_explained_variance(data, cum_expl_variance, opt_point, pc_greater)"],"metadata":{"id":"YmplBFhvwK8v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Gewichtung Features in den Hauptkomponenten\n","</font></p>"],"metadata":{"id":"mToEl929PsT0"}},{"cell_type":"markdown","source":["Die Werte in pca.components_ stellen die Gewichte oder Koeffizienten der urspr√ºnglichen Features in den jeweiligen Hauptkomponenten dar.\n","\n","Jede Zeile der Matrix entspricht einer Hauptkomponente.\n","Jede Spalte der Matrix entspricht einem urspr√ºnglichen Feature.\n","Der Wert an der Position (i, j) in der Matrix gibt das Gewicht des Features j in der Hauptkomponente i an.\n","Interpretation:\n","\n","+ Vorzeichen: Das Vorzeichen eines Gewichts (+ oder -) gibt die Richtung des Einflusses des Features auf die Hauptkomponente an. Ein positives Gewicht bedeutet, dass ein Anstieg des Feature-Werts zu einem Anstieg des Werts der Hauptkomponente f√ºhrt. Ein negatives Gewicht bedeutet, dass ein Anstieg des Feature-Werts zu einem Abfall des Werts der Hauptkomponente f√ºhrt.\n","\n","+ Betrag: Der Betrag eines Gewichts gibt die St√§rke des Einflusses des Features auf die Hauptkomponente an. Je gr√∂√üer der absolute Wert des Gewichts, desto st√§rker tr√§gt das entsprechende Feature zur Hauptkomponente bei.\n","\n","Zusammenfassend:\n","\n","+ Hohe positive Werte: Das Feature tr√§gt stark und positiv zur Hauptkomponente bei.\n","+ Hohe negative Werte: Das Feature tr√§gt stark und negativ zur Hauptkomponente bei.\n","+ Werte nahe Null: Das Feature hat wenig Einfluss auf die Hauptkomponente."],"metadata":{"id":"WwvmudsDPcbu"}},{"cell_type":"markdown","source":["**Beispiel:**\n","\n","Nehmen wir an, wir haben einen Datensatz mit zwei Features: \"Gr√∂√üe\" und \"Gewicht\". Wenn die PCA eine Hauptkomponente findet, die \"PCA1\" genannt wird, und das Feature \"Gr√∂√üe\" einen starken und negativen Beitrag zu dieser Hauptkomponente hat, bedeutet dies, dass gr√∂√üere Personen tendenziell einen niedrigeren Wert auf der \"PCA1\"-Hauptkomponente haben."],"metadata":{"id":"9nZvuff3UWPx"}},{"cell_type":"code","source":["feature = data.columns\n","DataFrame(model.components_, columns=feature)"],"metadata":{"id":"oOrnOXgWNtmb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5 | Deploy\n","---"],"metadata":{"id":"ekj9DgEXxjzf"}}]}